{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import array\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "\n",
    "    return (1 / (1 + numpy.exp(-x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 203) (1081, 24)\n"
     ]
    }
   ],
   "source": [
    "X = scipy.io.loadmat('../dataX.mat')['arr']\n",
    "Y = scipy.io.loadmat('../dataY.mat')['arr']\n",
    "# some daily information (5 weeks)\n",
    "data = X[:-8, :-35]\n",
    "Y = Y[8:, :-5]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SparseAutoencoder(object):\n",
    "    def __init__(self, visible_size, hidden_size, rho, lamda, beta):\n",
    "\n",
    "        \"\"\" Initialize parameters of the Autoencoder object \"\"\"\n",
    "\n",
    "        self.visible_size = visible_size    # number of input units\n",
    "        self.hidden_size = hidden_size      # number of hidden units\n",
    "        self.rho = rho                      # desired average activation of hidden units\n",
    "        self.lamda = lamda                  # weight decay parameter\n",
    "        self.beta = beta                    # weight of sparsity penalty term\n",
    "\n",
    "        \"\"\" Set limits for accessing 'theta' values \"\"\"\n",
    "\n",
    "        self.limit0 = 0\n",
    "        self.limit1 = hidden_size * visible_size\n",
    "        self.limit2 = 2 * hidden_size * visible_size\n",
    "        self.limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size\n",
    "\n",
    "        \"\"\" Initialize Neural Network weights randomly\n",
    "            W1, W2 values are chosen in the range [-r, r] \"\"\"\n",
    "\n",
    "        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1)\n",
    "\n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "\n",
    "        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))\n",
    "        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))\n",
    "\n",
    "        \"\"\" Bias values are initialized to zero \"\"\"\n",
    "\n",
    "        b1 = numpy.zeros((hidden_size, 1))\n",
    "        b2 = numpy.zeros((visible_size, 1))\n",
    "\n",
    "        \"\"\" Create 'theta' by unrolling W1, W2, b1, b2 \"\"\"\n",
    "\n",
    "        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),\n",
    "                                        b1.flatten(), b2.flatten()))\n",
    "        \n",
    "    def sparseAutoencoderCost(self, theta, input):\n",
    "        \"\"\" Returns gradient of 'theta' using Backpropagation algorithm \"\"\"\n",
    "\n",
    "        \"\"\" Extract weights and biases from 'theta' input \"\"\"\n",
    "\n",
    "        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)\n",
    "        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)\n",
    "        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)\n",
    "        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)\n",
    "\n",
    "        \"\"\" Compute output layers by performing a feedforward pass\n",
    "            Computation is done for all the training inputs simultaneously \"\"\"\n",
    "\n",
    "        hidden_layer = sigmoid(numpy.dot(W1, input) + b1)\n",
    "        output_layer = sigmoid(numpy.dot(W2, hidden_layer) + b2)\n",
    "\n",
    "        \"\"\" Estimate the average activation value of the hidden layers \"\"\"\n",
    "\n",
    "        rho_cap = numpy.sum(hidden_layer, axis = 1) / input.shape[1]\n",
    "\n",
    "        \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "\n",
    "        diff = output_layer - input\n",
    "\n",
    "        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1]\n",
    "        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) +\n",
    "                                                   numpy.sum(numpy.multiply(W2, W2)))\n",
    "        KL_divergence        = self.beta * numpy.sum(self.rho * numpy.log(self.rho / rho_cap) +\n",
    "                                                    (1 - self.rho) * numpy.log((1 - self.rho) / (1 - rho_cap)))\n",
    "        cost                 = sum_of_squares_error + weight_decay + KL_divergence\n",
    "\n",
    "        KL_div_grad = self.beta * (-(self.rho / rho_cap) + ((1 - self.rho) / (1 - rho_cap)))\n",
    "\n",
    "        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))\n",
    "        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out) + numpy.transpose(numpy.matrix(KL_div_grad)), \n",
    "                                 numpy.multiply(hidden_layer, 1 - hidden_layer))\n",
    "\n",
    "        \"\"\" Compute the gradient values by averaging partial derivatives\n",
    "            Partial derivatives are averaged over all training examples \"\"\"\n",
    "\n",
    "        W1_grad = numpy.dot(del_hid, numpy.transpose(input))\n",
    "        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))\n",
    "        b1_grad = numpy.sum(del_hid, axis = 1)\n",
    "        b2_grad = numpy.sum(del_out, axis = 1)\n",
    "\n",
    "        W1_grad = W1_grad / input.shape[1] + self.lamda * W1\n",
    "        W2_grad = W2_grad / input.shape[1] + self.lamda * W2\n",
    "        b1_grad = b1_grad / input.shape[1]\n",
    "        b2_grad = b2_grad / input.shape[1]\n",
    "\n",
    "        \"\"\" Transform numpy matrices into arrays \"\"\"\n",
    "\n",
    "        W1_grad = numpy.array(W1_grad)\n",
    "        W2_grad = numpy.array(W2_grad)\n",
    "        b1_grad = numpy.array(b1_grad)\n",
    "        b2_grad = numpy.array(b2_grad)\n",
    "\n",
    "        \"\"\" Unroll the gradient values and return as 'theta' gradient \"\"\"\n",
    "\n",
    "        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),\n",
    "                                        b1_grad.flatten(), b2_grad.flatten()))\n",
    "\n",
    "        return [cost, theta_grad]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "        \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = getGroundTruth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedForwardAutoencoder(theta, hidden_size, visible_size, input):\n",
    "    \"\"\" Returns the hidden layer activations of the Autoencoder \"\"\"\n",
    "\n",
    "    \"\"\" Define limits to access useful data \"\"\"\n",
    "\n",
    "    limit0 = 0\n",
    "    limit1 = hidden_size * visible_size\n",
    "    limit2 = 2 * hidden_size * visible_size\n",
    "    limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "    \n",
    "    \"\"\" Access W1 and b1 from 'theta' \"\"\"\n",
    "    \n",
    "    W1 = theta[limit0 : limit1].reshape(hidden_size, visible_size)\n",
    "    b1 = theta[limit2 : limit3].reshape(hidden_size, 1)\n",
    "    \n",
    "    \"\"\" Compute the hidden layer activations \"\"\"\n",
    "    \n",
    "    hidden_layer = 1 / (1 + numpy.exp(-(numpy.dot(W1, input) + b1)))\n",
    "    \n",
    "    return hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Returns a row of Stacked Autoencoder parameters \"\"\"\n",
    "\n",
    "def stack2Params(stack):\n",
    "\n",
    "    \"\"\" Initialize an empty list of parameters \"\"\"\n",
    "\n",
    "    params = []\n",
    "    num_layers = len(stack) / 2\n",
    "\n",
    "    \"\"\" For each layer in the neural network, append the corresponding parameters \"\"\"\n",
    "\n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        params = numpy.concatenate((params, numpy.array(stack[i, \"W\"]).flatten()))\n",
    "        params = numpy.concatenate((params, numpy.array(stack[i, \"b\"]).flatten()))\n",
    "        \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Returns a stack of Stacked Autoencoder parameters \"\"\"\n",
    "\n",
    "def params2Stack(params, net_config):\n",
    "\n",
    "    \"\"\" Initialize an empty stack \"\"\"\n",
    "\n",
    "    stack  = {}\n",
    "    limit0 = 0\n",
    "    \n",
    "    for i in range(len(net_config)-2):\n",
    "    \n",
    "        \"\"\" Calculate limits of layer parameters, using neural network config \"\"\"\n",
    "    \n",
    "        limit1 = limit0 + net_config[i] * net_config[i+1]\n",
    "        limit2 = limit1 + net_config[i+1]\n",
    "        \n",
    "        \"\"\" Extract layer parameters, and store in the stack \"\"\"\n",
    "        \n",
    "        stack[i, \"W\"] = params[limit0 : limit1].reshape(net_config[i+1], net_config[i])\n",
    "        stack[i, \"b\"] = params[limit1 : limit2].reshape(net_config[i+1], 1)\n",
    "        \n",
    "        limit0 = limit2\n",
    "        \n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stackedAutoencoderCost(theta, net_config, lamda, data, labels=data):\n",
    "\n",
    "    \"\"\" Calculate limits for Softmax parameters \"\"\"\n",
    "\n",
    "    input_size   = net_config[-2]\n",
    "    num_classes  = net_config[-1]\n",
    "    \n",
    "    limit0 = 0\n",
    "    limit1 = num_classes * input_size\n",
    "    \n",
    "    \"\"\" Extract Softmax and layer parameters \"\"\"\n",
    "    \n",
    "    softmax_theta = theta[limit0 : limit1].reshape(num_classes, input_size)\n",
    "    stack         = params2Stack(theta[limit1 :], net_config)\n",
    "    \n",
    "    num_layers = len(stack) / 2\n",
    "    \n",
    "    \"\"\" Calculate activations for every layer \"\"\"\n",
    "    \n",
    "    activation    = {}\n",
    "    activation[0] = data\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        activation[i+1] = sigmoid(numpy.dot(stack[i, \"W\"], activation[i]) + stack[i, \"b\"])\n",
    "        \n",
    "    \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "    ground_truth = getGroundTruth(labels)\n",
    "    \n",
    "    \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "    \n",
    "    theta_x       = numpy.dot(softmax_theta, activation[num_layers])\n",
    "    hypothesis    = numpy.exp(theta_x)      \n",
    "    probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "    \n",
    "    \"\"\" Compute the traditional cost term \"\"\"\n",
    "    \n",
    "    cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "    traditional_cost = -(numpy.sum(cost_examples) / data.shape[1])\n",
    "    \n",
    "    \"\"\" Compute the weight decay term \"\"\"\n",
    "    \n",
    "    theta_squared = numpy.multiply(softmax_theta, softmax_theta)\n",
    "    weight_decay  = 0.5 * lamda * numpy.sum(theta_squared)\n",
    "    \n",
    "    \"\"\" Add both terms to get the cost \"\"\"\n",
    "    \n",
    "    cost = traditional_cost + weight_decay\n",
    "    \n",
    "    \"\"\" Compute Softmax 'theta' gradient \"\"\"\n",
    "    \n",
    "    softmax_theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(activation[num_layers]))\n",
    "    softmax_theta_grad = softmax_theta_grad / data.shape[1] + lamda * softmax_theta\n",
    "    \n",
    "    \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "    \n",
    "    delta = {}    \n",
    "    delta[num_layers] = -numpy.multiply(numpy.dot(numpy.transpose(softmax_theta), ground_truth - probabilities),\n",
    "                                        numpy.multiply(activation[num_layers], 1 - activation[num_layers]))\n",
    "    for i in range(num_layers-1):\n",
    "    \n",
    "        index        = num_layers - i - 1\n",
    "        delta[index] = numpy.multiply(numpy.dot(numpy.transpose(stack[index, \"W\"]), delta[index+1]),\n",
    "                                       numpy.multiply(activation[index], 1 - activation[index]))\n",
    "                                       \n",
    "    \"\"\" Compute the partial derivatives, with respect to the layer parameters \"\"\"\n",
    "                                       \n",
    "    stack_grad = {}\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        index = num_layers - i - 1\n",
    "        stack_grad[index, \"W\"] = numpy.dot(delta[index+1], numpy.transpose(activation[index])) / data.shape[1]\n",
    "        stack_grad[index, \"b\"] = numpy.sum(delta[index+1], axis = 1) / data.shape[1]\n",
    "    \n",
    "    \"\"\" Concatenate the gradient values and return as 'theta' gradient \"\"\"\n",
    "        \n",
    "    params_grad = stack2Params(stack_grad)\n",
    "    theta_grad  = numpy.concatenate((numpy.array(softmax_theta_grad).flatten(),\n",
    "                                     numpy.array(params_grad).flatten()))\n",
    "    \n",
    "    return [cost, theta_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stackedAutoencoderPredict(theta, net_config, data):\n",
    "\n",
    "    \"\"\" Calculate limits for Softmax parameters \"\"\"\n",
    "\n",
    "    input_size   = net_config[-2]\n",
    "    num_classes  = net_config[-1]\n",
    "    \n",
    "    limit0 = 0\n",
    "    limit1 = num_classes * input_size\n",
    "    \n",
    "    \"\"\" Extract Softmax and layer parameters \"\"\"\n",
    "    \n",
    "    softmax_theta = theta[limit0 : limit1].reshape(num_classes, input_size)\n",
    "    stack         = params2Stack(theta[limit1 :], net_config)\n",
    "    \n",
    "    num_layers = len(stack) / 2\n",
    "    \n",
    "    \"\"\" Calculate the activations of the final layer \"\"\"\n",
    "    \n",
    "    activation = data\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "    \n",
    "        activation = sigmoid(numpy.dot(stack[i, \"W\"], activation) + stack[i, \"b\"])\n",
    "        \n",
    "    \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "    theta_x       = numpy.dot(softmax_theta, activation)\n",
    "    hypothesis    = numpy.exp(theta_x)      \n",
    "    \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getGroundTruth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-82782f8237b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy after finetuning :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mexecuteStackedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-82782f8237b6>\u001b[0m in \u001b[0;36mexecuteStackedAutoencoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     opt_solution      = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n\u001b[1;32m     56\u001b[0m                                                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msae2_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'L-BFGS-B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                                                 jac = True, options = {'maxiter': max_iterations})\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0msoftmax_opt_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_solution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/m/anaconda/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 450\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/Users/m/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/m/anaconda/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/m/anaconda/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/m/anaconda/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e8513b2a49c7>\u001b[0m in \u001b[0;36msoftmaxCost\u001b[0;34m(self, theta, input, labels)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"\"\" Compute the groundtruth matrix \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetGroundTruth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"\"\" Reshape 'theta' for ease of computation \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getGroundTruth' is not defined"
     ]
    }
   ],
   "source": [
    "def executeStackedAutoencoder():\n",
    "    \n",
    "    \"\"\" Define the parameters of the first Autoencoder \"\"\"\n",
    "    \n",
    "    visible_size   = 1081    # size of input vector\n",
    "    hidden_size1   = 100    # size of hidden layer vector of first autoencoder\n",
    "    hidden_size2   = 100    # size of hidden layer vector of second autoencoder\n",
    "    rho            = 0.1    # desired average activation of hidden units\n",
    "    lamda          = 0.003  # weight decay parameter\n",
    "    beta           = 3      # weight of sparsity penalty term\n",
    "    max_iterations = 200    # number of optimization iterations\n",
    "    num_classes    = 1      # number of classes\n",
    "    \n",
    "    \"\"\" Load MNIST images for training and testing \"\"\"\n",
    "    \n",
    "    train_data    = data\n",
    "    train_labels  = data\n",
    "    \n",
    "    \"\"\" Initialize the first Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder1 = SparseAutoencoder(visible_size, hidden_size1, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution   = scipy.optimize.minimize(encoder1.sparseAutoencoderCost, encoder1.theta, \n",
    "                                             args = (train_data,), method = 'L-BFGS-B', \n",
    "                                             jac = True, options = {'maxiter': max_iterations})\n",
    "    sae1_opt_theta = opt_solution.x\n",
    "    \n",
    "    \"\"\" Get the features corresponding to first Autoencoder \"\"\"\n",
    "    \n",
    "    sae1_features = feedForwardAutoencoder(sae1_opt_theta, hidden_size1, visible_size, train_data)\n",
    "    \n",
    "    \"\"\" Initialize the second Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder2 = SparseAutoencoder(hidden_size1, hidden_size2, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution   = scipy.optimize.minimize(encoder2.sparseAutoencoderCost, encoder2.theta, \n",
    "                                             args = (sae1_features,), method = 'L-BFGS-B', \n",
    "                                             jac = True, options = {'maxiter': max_iterations})\n",
    "    sae2_opt_theta = opt_solution.x\n",
    "    \n",
    "    \"\"\" Get the features corresponding to second Autoencoder \"\"\"\n",
    "    \n",
    "    sae2_features = feedForwardAutoencoder(sae2_opt_theta, hidden_size2, hidden_size1, sae1_features)\n",
    "    \n",
    "    \"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "    \n",
    "    regressor = SoftmaxRegression(hidden_size2, num_classes, lamda)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution      = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                                args = (sae2_features, train_labels,), method = 'L-BFGS-B', \n",
    "                                                jac = True, options = {'maxiter': max_iterations})\n",
    "    softmax_opt_theta = opt_solution.x\n",
    "    \n",
    "    \"\"\" Create a stack of the Stacked Autoencoder parameters \"\"\"\n",
    "    \n",
    "    stack = {}\n",
    "    stack[0, \"W\"] = sae1_opt_theta[encoder1.limit0 : encoder1.limit1].reshape(hidden_size1, visible_size)\n",
    "    stack[1, \"W\"] = sae2_opt_theta[encoder2.limit0 : encoder2.limit1].reshape(hidden_size2, hidden_size1)\n",
    "    stack[0, \"b\"] = sae1_opt_theta[encoder1.limit2 : encoder1.limit3].reshape(hidden_size1, 1)\n",
    "    stack[1, \"b\"] = sae2_opt_theta[encoder2.limit2 : encoder2.limit3].reshape(hidden_size2, 1)\n",
    "    \n",
    "    \"\"\" Create a vector of the Stacked Autoencoder parameters for optimization \"\"\"\n",
    "    \n",
    "    stack_params     = stack2Params(stack)\n",
    "    stacked_ae_theta = numpy.concatenate((softmax_opt_theta.flatten(), stack_params.flatten()))\n",
    "    \n",
    "    \"\"\" Create a neural network configuration, with number of units in each layer \"\"\"\n",
    "    \n",
    "    net_config = [visible_size, hidden_size1, hidden_size2, num_classes]\n",
    "    \n",
    "    \"\"\" Load MNIST test images and labels \"\"\"\n",
    "    \n",
    "#     test_data   = loadMNISTImages('t10k-images.idx3-ubyte') \n",
    "#     test_labels = loadMNISTLabels('t10k-labels.idx1-ubyte')\n",
    "    \n",
    "    \"\"\" Get predictions after greedy training \"\"\"\n",
    "    \n",
    "    predictions = stackedAutoencoderPredict(stacked_ae_theta, net_config, test_data)\n",
    "    \n",
    "    \"\"\" Print accuracy of the trained model \"\"\"\n",
    "    \n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print( \"Accuracy after greedy training :\", numpy.mean(correct))\n",
    "    \n",
    "    \"\"\" Finetune the greedily trained model \"\"\"\n",
    "    \n",
    "    opt_solution = scipy.optimize.minimize(stackedAutoencoderCost, stacked_ae_theta, \n",
    "                                           args = (net_config, lamda, train_data, train_labels,),\n",
    "                                           method = 'L-BFGS-B', jac = True, options = {'maxiter': max_iterations})\n",
    "    stacked_ae_opt_theta = opt_solution.x\n",
    "    \n",
    "    \"\"\" Get predictions after finetuning \"\"\"\n",
    "    \n",
    "    predictions = stackedAutoencoderPredict(stacked_ae_opt_theta, net_config, test_data)\n",
    "    \n",
    "    \"\"\" Print accuracy of the trained model \"\"\"\n",
    "    \n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print(\"Accuracy after finetuning :\", numpy.mean(correct))\n",
    "    \n",
    "executeStackedAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

@article{Abdi2003,
abstract = {PLS regression is a recent technique that generalizes and combines features from principal component analysis and multiple regression. Its goal is to predict or analyze a set of dependent variables from a set of independent variables or predictors. This prediction is achieved by extracting from the predictors a set of orthogonal factors called latent variables which have the best predictive power.},
author = {Abdi, Herv{\'{e}}},
doi = {http://dx.doi.org/10.4135/9781412950589.n690},
file = {:Users/m/Desktop/mipt/семестр 8/нир/Abdi-PLS-regration.pdf:pdf},
isbn = {9781412950589},
issn = {15315487},
journal = {Encyclopedia for research methods for the social sciences},
pages = {792--795},
pmid = {20539106},
title = {{Partial Least Squares (PLS) Regression}},
year = {2003}
}
@book{Adolph,
annote = {NULL},
author = {Adolph, Karen E},
booktitle = {Action As an Organizer of Learning and Development},
file = {:Users/m/Desktop/mipt/семестр 8/нир/Learning to learn:},
isbn = {9781461375272},
title = {{Learning to learn}}
}
@article{Bulut2014,
author = {Bulut, Elif and Egrioglu, Erol},
doi = {10.5923/j.ajis.20140404.05},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/PLS{\_}ElmamNN.pdf:pdf},
keywords = {19,activation functions,based on logistic activation,based on radial bases,elman neural network,feed forward neural network,function and particle swarm,optimization methods,partial least squares regression,prediction,propose non-linear pls method,xufeng,zhou et al},
number = {4},
pages = {154--158},
title = {{A New Partial Least Square Method Based on Elman Neural Network}},
volume = {4},
year = {2014}
}
@article{Caruana2003,
abstract = {In supervised learning variable selection is used to find a subset of the available inputs that accurately predict the output. This paper shows that some of the variables that variable selection discards can beneficially be used as extra outputs for inductive transfer. Using discarded input variables as extra outputs forces the model to learn mappings from the variables that were selected as inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available to the model that is being trained on the main output, often resulting in improved performance on that main output. We present three synthetic problems (two regression problems and one classification problem) where performance improves if some variables discarded by variable selection are used as extra outputs. We then apply variable selection to two real problems (DNA splice-junction and pneumonia risk prediction) and demonstrate the same effect: using some of the discarded input variables as extra outputs yields somewhat better performance on both of these problems than can be achieved by variable selection alone. This new approach enhances the benefit of variable selection by allowing the learner to benefit from variables that would otherwise have been discarded by variable selection, but without suffering the loss in performance that occurs when these variables are used as inputs.},
author = {Caruana, Rich and de Sa, Virginia R.},
doi = {10.1162/153244303322753652},
file = {:Users/m/Desktop/mipt/семестр 8/нир/caruana03a.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {inductive transfer,multitask learning,output variable selection},
number = {7-8},
pages = {1245--1264},
title = {{Benefitting from the Variables that Variable Selection Discards}},
volume = {3},
year = {2003}
}
@article{Evgeniou2007,
author = {Evgeniou, A A T and Pontil, M},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/Multi-Task{\_}Feature{\_}Learning.pdf:pdf},
isbn = {9781450342322},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {41},
title = {{Multi-task feature learning}},
volume = {19},
year = {2007}
}
@article{Frank1990,
abstract = {Frank, I.E., 1990. A nonlinear PLS model. Chemometrics and Intelligent Laboratory Systems, 8: 109-119. A nonlinear extension of the PLS (partial least squares regression) method is introduced. The algorithm connects the predictor and response latent variables by a smooth but otherwise unrestricted nonlinear function. Similarities and differences between the linear and nonlinear PLS models are discussed. The performance of the new nonlinear PLS method is illustrated on three chemical data sets with different covariance structures. ?? 1990.},
author = {Frank, Ildiko E.},
doi = {10.1016/0169-7439(90)80128-S},
file = {:Users/m/Downloads/frank1990.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
number = {2},
pages = {109--119},
pmid = {1280},
title = {{A nonlinear PLS model}},
volume = {8},
year = {1990}
}
@article{Geladi1988,
abstract = {The growing awareness of the need for non-deterministic and distribution-free (soft) models combined with an iterative algorithm for finding latent variables led to the construction of partial least squares models. There have been separate developments in the humanistic and the natural sciences, with stress on different aspects and a different terminology. The historical development is described and some key topics are explained.},
author = {Geladi, Paul},
doi = {http://doi.wiley.com/10.1002/cem.1180020403},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/geladi1988.pdf:pdf},
isbn = {1099-128X},
issn = {1099-128X},
journal = {Journal of Chemometrics},
keywords = {History of PLS,partial least squares models},
number = {January},
pages = {231--246},
title = {{Notes on the history and nature of partial least squares (PLS) modelling}},
volume = {2},
year = {1988}
}
@article{Hoskuldsson1988,
abstract = {In this paper we develop the mathematical and statistical structure of PLS regression. We show the PLS regression algorithm and how it can be interpreted in model building. The basic mathematical principles that lie behind two block PLS are depicted. We also show the statistical aspects of the PLS method when it is used for model building. Finally we show the structure of the PLS decompositions of the data matrices involved},
author = {H{\"{o}}skuldsson, Agnar},
doi = {10.1002/cem.1180020306},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/hskuldsson1988.pdf:pdf},
isbn = {1099-128X},
issn = {1099-128X},
journal = {Journal of Chemometrics},
keywords = {component regression,covariance decomposition,partial least squares},
number = {August 1987},
pages = {581--591},
title = {{PLS regression.}},
volume = {2},
year = {1988}
}
@article{Lehky2014,
abstract = {We have calculated the intrinsic dimensionality of visual object representations in anterior inferotemporal (AIT) cortex, based on responses of a large sample of cells stimulated with photographs of diverse objects. As dimensionality was dependent on data set size, we determined asymptotic dimensionality as both the number of neurons and number of stimulus image approached infinity. Our final dimensionality estimate was 93 (SD: ± 11), indicating that there is basis set of approximately a hundred independent features that characterize the dimensions of neural object space. We believe this is the first estimate of the dimensionality of neural visual representations based on single-cell neurophysiological data. The dimensionality of AIT object representations was much lower than the dimensionality of the stimuli. We suggest that there may be a gradual reduction in the dimensionality of object representations in neural populations going from retina to inferotemporal cortex, as receptive fields become increasingly complex. Introduction},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1309.2848v1},
author = {Lehky, Sidney R. and Kiani, Roozbeh and Esteky, Hossein and Tanaka, Keiji},
doi = {10.1162/NECO},
eprint = {1309.2848v1},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/Vidaurre2013{\_}Bayesian{\_}Sparse{\_}PLS.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural computation},
number = {10},
pages = {1840--1872},
pmid = {25602775},
title = {{Dimensionality of object representations in monkey inferotemporal cortex}},
volume = {1872},
year = {2014}
}
@article{Lu2004,
annote = {NULL},
author = {Lu, Wen-Cong and Chen, Nian-Yi and Li, Guo-Zheng and Yang, Jie},
file = {:Users/m/Desktop/mipt/семестр 8/нир/ mlt{\_}using{\_}pls.pdf:pdf},
journal = {Proceedings of the Seventh International Conference on Information Fusion; International Society of Information Fusion},
keywords = {feature,multitask learning,multivariate calibration,partial least squares,selection},
pages = {79--84},
title = {{Multitask Learning Using Partial Least Squares Method}},
volume = {1},
year = {2004}
}
@article{Mcavovt1992,
author = {Mcavovt, J and Process, Chemical},
file = {:Users/m/Downloads/qin1992.pdf:pdf},
number = {4},
pages = {379--391},
title = {{USING}},
volume = {16},
year = {1992}
}
@article{Varnek2012,
abstract = {This paper is focused on modern approaches to machine learning, most of which are as yet used infrequently or not at all in chemoinformatics. Machine learning methods are characterized in terms of the "modes of statistical inference" and "modeling levels" nomenclature and by considering different facets of the modeling with respect to input/ouput matching, data types, models duality, and models inference. Particular attention is paid to new approaches and concepts that may provide efficient solutions of common problems in chemoinformatics: improvement of predictive performance of structure-property (activity) models, generation of structures possessing desirable properties, model applicability domain, modeling of properties with functional endpoints (e.g., phase diagrams and dose-response curves), and accounting for multiple molecular species (e.g., conformers or tautomers).},
annote = {NULL},
author = {Varnek, Alexandre and Baskin, Igor},
doi = {10.1021/ci200409x},
file = {:Users/m/Desktop/mipt/семестр 8/нир/literature/ML{\_}chemoinformatics{\_}review{\_}2012.pdf:pdf},
isbn = {1549-9596},
issn = {15499596},
journal = {Journal of Chemical Information and Modeling},
number = {6},
pages = {1413--1437},
pmid = {22582859},
title = {{Machine learning methods for property prediction in chemoinformatics: Quo Vadis?}},
volume = {52},
year = {2012}
}
@article{Xuefeng2010,
abstract = {A novel hybrid artificial neural network (HANN) integrating error back propagation algorithm (BP) with partial least square regression (PLSR) was proposed to overcome two main flaws of artificial neural network (ANN), i.e. tendency to overfitting and difficulty to determine the optimal number of the hidden nodes. Firstly, single-hidden-layer network consisting of an input layer, a single hidden layer and an output layer is selected by HANN. The number of the hidden-layer neurons is determined according to the number of the modeling samples and the number of the neural network parameters. Secondly, BP is employed to train ANN, and then the hidden layer is applied to carry out the nonlinear transformation for independent variables. Thirdly, the inverse function of the output-layer node activation function is applied to calculate the expectation of the output-layer node input, and PLSR is employed to identify PLS components from the nonlinear transformed variables, remove the correlation among the nonlinear transformed variables and obtain the optimal relationship model of the nonlinear transformed variables with the expectation of the output-layer node input. Thus, the HANN model is developed. Further, HANN was employed to develop naphtha dry point soft sensor and the most important intermediate product concentration (i.e. 4-carboxybenzaldehyde concentration) soft sensor in p-xylene (PX) oxidation reaction due to the fact that there exist many factors having nonlinear effect on them and significant correlation among their factors. The results of two HANN applications show that HANN overcomes overfitting and has the robust character. And, the predicted squared relative errors of two optimal HANN models are all lower than those of two optimal ANN models and the mean predicted squared relative errors of HANN are lower than those of ANN in two applications. ?? 2010 Elsevier B.V.},
author = {Xuefeng, Yan},
doi = {10.1016/j.chemolab.2010.07.002},
file = {:Users/m/Downloads/xuefeng2010.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Artificial neural network,Correlation,Error back propagation,Partial least square regression,Soft sensor},
number = {2},
pages = {152--159},
publisher = {Elsevier B.V.},
title = {{Hybrid artificial neural network based on BP-PLSR and its application in development of soft sensors}},
volume = {103},
year = {2010}
}
@article{Yan2003,
abstract = {A novel genetic algorithm (GA) including chaotic variable named chaos-genetic algorithm (CGA) was proposed. Due to the nature of chaotic variable, i.e. pseudo-randomness, ergodicity and irregularity, the evolutional process of CGA makes the individuals of subgenerations distributed ergodically in the defined space and circumvents the premature of the individuals of subgenerations. The performance of CGA was demonstrated through two examples and compared with that by the traditional genetic algorithms (TGA). The results showed the superior performances of CGA over TGA, and moreover, the probability of finding the global optimal value by using CGA is larger than that by using TGA. To illustrate the performance of CGA further, it was employed to optimize the operating conditions of aromatic hydrocarbon isomerization (AHI) process modeled by the radial basis functions (RBF) coupled with partial least squares (PLS) approach. Satisfactory results were obtained. Further, a generalized methodology, which employs RBF-PLS approach to model the complex chemical process based upon practical observation data and subsequently applies CGA to find the optimal operating conditions, was suggested too. ?? 2003 Elsevier Science Ltd. All rights reserved.},
author = {Yan, Xuefeng F. and Chen, Dezhao Z. and Hu, Shangxu X.},
doi = {10.1016/S0098-1354(03)00074-7},
file = {:Users/m/Downloads/yan2003.pdf:pdf},
isbn = {0098-1354},
issn = {00981354},
journal = {Computers and Chemical Engineering},
keywords = {Chaos-genetic algorithms,Chaotic variable,Model,Optimization,Partial least squares,Radial basis functions},
number = {10},
pages = {1393--1404},
title = {{Chaos-genetic algorithms for optimizing the operating conditions based on RBF-PLS model}},
volume = {27},
year = {2003}
}
@article{Zhou2007,
abstract = {In the present study a new version of nonlinear partial least-square method based on artificial neural network transformation (ANN-NLPLS) has been proposed. This algorithm firstly transforms the training descriptors into the hidden layer outputs using the universal nonlinear mapping carried by an artificial neural network, and then utilizes PLS to relate the outputs of the hidden layer to the bioactivities. The weights between the input and hidden layers are optimized by a particle swarm optimization (PSO) method using the criterion of minimized model error via PLS modeling. An F-statistic is introduced to determine automatically the number of PLS components during the weight optimization. The performance is assessed using a simulated data set and two quantitative structure-activity relation (QSAR) data sets. Results of these three data sets demonstrate that ANN-NLPLS offers enhanced capacity in modeling nonlinearity while circumventing the overfitting frequently involved in nonlinear modeling. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Zhou, Yan Ping and Jiang, Jian Hui and Lin, Wei Qi and Xu, Lu and Wu, Hai Long and Shen, Guo Li and Yu, Ru Qin},
doi = {10.1016/j.talanta.2006.05.058},
file = {:Users/m/Downloads/zhou2007.pdf:pdf},
isbn = {0039-9140},
issn = {00399140},
journal = {Talanta},
keywords = {2-Aryl(heteroaryl)-2,5-dihydropyrazolo[4,3-c]quinolin-3-(3H)-ones,Antitumor agents,Artificial neural network transformation-based nonlinear partial least-square,Particle swarm optimization,QSAR},
number = {2},
pages = {848--853},
pmid = {19071384},
title = {{Artificial neural network-based transformation for nonlinear partial least-square regression with application to QSAR studies}},
volume = {71},
year = {2007}
}

\documentclass[12pt,twoside]{article}
\usepackage[russian,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{abstract}
\usepackage{amsmath,amssymb,mathrsfs,mathtext,amsthm}
\usepackage{a4wide}
\usepackage[T2A]{fontenc}
\usepackage{subfig}
\usepackage{url}
\usepackage[usenames]{color}
\usepackage{colortbl}

\newcommand{\hdir}{.}
\usepackage{hyperref}       % clickable links
\usepackage{lineno}
\usepackage{graphicx,multicol}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
\newtheorem{theorem}{Теорема}
\newtheorem{statement}{Утверждение}
\usepackage{algorithm}
\usepackage[noend]{algcompatible}

%\renewcommand{\baselinestretch}{1.4}


\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}

\begin{document}
	\title
	{Dimensionality reduction for time series decoding and forecasting problems\thanks{The work was financially supported by the Russian Foundation for Basic Research (project \textcolor{red}{16-07-01155}).}}
	\date{}
	\maketitle
	\begin{center}
		R.\,V.~Isachenko\footnote{Moscow Institute of Physics and Technology, isa-ro@yandex.ru},
		M.\,R.~Vladimirova\footnote{Moscow Institute of Physics and Technology, vladimirova.maria@phystech.edu},
		V.\,V.~Strijov\footnote{A. A. Dorodnicyn Computing Centre, Federal Research Center “Computer Science and Control” of the Russian Academy of Sciences, strijov@ccas.ru}
	\end{center}
	\textbf{Abstract:} 
	The paper is devoted to the problem of detecting the relation between independent and target variables.
	We propose to predict a multidimensional target vector instead of predicting one timestamp point.
	We consider the linear model of partial least squares (PLS).
	The method finds the matrix of a joint description for the design matrix and the outcome matrix.
	The description is low-dimensional and allows to build a simple, stable model.
	We conducted computational experiments on the real data of energy consumption and electrocorticograms signals (ECoG).
	
	\bigskip
	\textbf{Keywords}: time series decoding, forecast, partial least squares, dimensionality reduction

\linenumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Paper investigates the problem of dependence recovering between an input data and a model outcome.
The proposed model is suitable for predicting a multidimensional target variable.
In the case of the forecasting problem the object and target spaces have the same nature.
To build the model we need to construct autoregressive matrices for input objects and target variables.
The object is a local signal history, the outcome is signal values in the next timestamps.
An autoregressive model makes a consumption that the current signal values depend linearly on the previous signal values.

In the case of time series decoding problem the object and target spaces are different in nature, the outcome is a system response to the input signal.
The autoregressive design matrix contains local history of the input signal.
The autoregressive target matrix contains the local history of the response.

The object space in time series decoding problems is high dimensional.
Excessive dimensionality of the feature description leads to instability of the model.
To solve this problem the feature selection procedures are used~\cite{katrutsa2015qpfs,li2016feature}.

The paper considers the partial least squares regression (PLS) model~\cite{wegelin2000survey,abdi2003pls,geladi1986partial}.
The PLS model reduces the dimension of the input data and extracts the linear combination of features which have the greatest impact on the response vector.
Feature extraction is an iterative process in order of decreasing the influence on the response vector.
PLS regression methods are described in detail in~\cite{geladi1988pls, hoskuldsson1988plsr,de1993simpls}.
The difference between various PLS approaches, different kinds of the PLS regression could be found in~\cite{rosipal2006overview}.

The current state of the field and the overview of nonlinear PLS method modifications are described in~\cite{rosipal2011npls}.
A nonlinear PLS method extension was introduced in~\cite{wold1989nonlinear}.
There has been developed the variety of PLS modifications.
The proposed nonlinear PLS methods are based on smoothing splines~\cite{frank1990npls}, neural networks~\cite{qin1992npls}, radial basis functions~\cite{yan2003geneticpls}, genetic algorithms~\cite{hiden1998geneticpls}.

The result of the feature selection is the dimensionality reduction and the increasing model stability without significant loss of the prediction quality.
The proposed method is used on two datasets with the redundant input and target spaces.
The first dataset consists of hourly time series of energy consumption. 
Time series were collected in Poland from 1999 to 2004.

The second dataset comes from the NeuroTycho project~\cite{neurotycho} that designs Brain-Computer Interface (BCI)~\cite{millan2010combining,mason2007comprehensive} for information transmitting between brains and electronic devices.
Brain-Computer Interface (BCI) system enhances its user’s mental and physical abilities, providing a direct communication mean between the brain and a computer~\cite{millan2004brain}. 
BCIs aim at restoring damaged functionality of motorically or cognitively impaired patients.
The goal of motor imagery analysis is to recognize intended movements from the recorded brain activity. 
While there are various techniques for measuring cortical data for BCI~\cite{nicolas2012brain,amiri2013review}, we concentrate on the ElectroCorticoGraphic (ECoG) signals~\cite{eliseyev2016penalized}. 
ECoG, as well as other invasive techniques, provides more stable recordings and better resolution in temporal and spatial domains than its non-invasive counterparts.
We address the problem of continuous hand trajectory reconstruction. 
The subdural ECoG signals are measured across 32 channels as the subject is moving its hand.
Once the ECoG signals are transformed into informative features, the problem of trajectory reconstruction is an autoregression problem. 
Feature extraction involves application of some spectro-temporal transform to the ECoG signals from each channel~\cite{gasanov2017pls}.

In papers which are devoted to forecasting of complex spatial time series the forecast is built pointwise~\cite{box2015time,zhang2003time}.
If one need to predict multiple points simultaneously, it is proposed to compute forecasted points sequentially.
During this process the previous predicted values are used to obtain a subsequent ones.
The proposed method allows to obtain multiple predicted time series values at the same time taking into account hidden dependencies not only in the object space, but also in the target space.
The proposed method significantly reduces the dimensionality of the feature space.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Задана выборка $\mathfrak{D}= \left( \bX, \bY \right)$, где $\mathbf{X} \in \mathbb{R}^{m \times n}$~--- матрица объектов, $\mathbf{Y} \in \mathbb{R}^{m \times r}$~--- матрица ответов. 
Способ построения выборки под определенную прикладную задачу описан в разделе \textit{Вычислительный эксперимент}.

Предполагается, что между объектами $\bx \in \mathbb{R}^n$ и ответами $\by \in \mathbb{R}^r$ существует линейная зависимость 
\begin{equation}
 \underset{1 \times r}{\by} = \underset{1 \times n}{\vphantom{\by}\bx} \cdot \underset{n \times r}{\vphantom{\by}\bTheta} + \underset{1 \times r}{\vphantom{\by}\boldsymbol{\varepsilon}}, 
\label{eq::model}
\end{equation}
где $\bTheta$~---~матрица параметров модели, а~$\boldsymbol{\varepsilon}$~---~вектор регрессионных остатков.

Необходимо по известной выборке $\mathfrak{D}$ восстановить матрицу параметров модели~\eqref{eq::model}.
Оптимальные параметры находятся минимизацией функции ошибки.
Введем квадратичную функцию ошибки $S$ на выборке $\mathfrak{D}$:
\begin{equation}
	S(\bTheta | \mathfrak{D}) = {\left\| \underset{m \times n}{\vphantom{\by}\mathbf{X}} \cdot \underset{n \times r}{\vphantom{\by}\bTheta} - \underset{m \times r}{\vphantom{\by}\mathbf{Y}} \right\| }_2^2 = \sum_{i=1}^m \left\| \underset{1 \times n}{\vphantom{\by}\bx_i} \cdot \underset{n \times r}{\vphantom{\by}\bTheta} - \underset{1 \times r}{\vphantom{\by}\by_i} \right\|_2^2 \rightarrow\min_{\bTheta}.
\label{eq::error_function}
\end{equation}
 
Линейная зависимость столбцов матрицы $\bX$ приводит к неустойчивому решению задачи оптимизации~\eqref{eq::error_function}. Для устранения линейной зависимости применяются методы отбора признаков.

\section{Partial Least Squares method}

Для устранения линейной зависимости и снижения размерности пространства применяется метод главных компонент PCA. 
Основным недостатком данного метода является то, что он не учитывает взаимосвязь между объектами и ответами.
Метод частных наименьших квадратов PLS проецирует матрицу объектов $\bX$ и матрицу ответов $\bY$ в латентное пространство $\mathbb{R}^l$ меньшей размерности ($l < r < n$).
Алгоритм PLS находит в латентном пространстве матрицу $\bT \in \mathbb{R}^{m \times l}$, наилучшим образом описывающую исходные матрицы $\bX$ и $\bY$.

Матрица объектов $\bX$ и матрица ответов $\bY$ проецируются в латентное пространство следующим образом: 
\begin{align}
\label{eq::PLS_X}
 \underset{m \times n}{\vphantom{\bQ}\bX} 
 &= \underset{m \times l}{\vphantom{\bQ}\bT} \cdot \underset{l \times n}{\vphantom{\bQ}\bP^{\T}} + \underset{m \times n}{\vphantom{\bQ}\bF} 
 = \sum_{k=1}^l \underset{m \times 1}{\vphantom{\bp_k^{\T}}\bt_k} \cdot \underset{1 \times n}{\bp_k^{\T}} + \underset{m \times n}{\vphantom{\bp_k^{\T}}\bF},\\
 \label{eq::PLS_Y}
 \underset{m \times r}{\vphantom{\bQ}\bY} 
 &= \underset{m \times l}{\vphantom{\bQ}\bT} \cdot \underset{l \times r}{\bQ^{\T}} + \underset{m \times r}{\vphantom{\bQ}\bE}
 =  \sum_{k=1}^l  \underset{m \times 1}{\vphantom{\bq_k^{\T}}\bt_k} \cdot \underset{1 \times r}{\bq_k^{\T}} +  \underset{m \times r}{\vphantom{\bq_k^{\T}}\bE},
\end{align}
где $\bT$~--- матрица совместного описания объектов и ответов в латентном пространстве, причём столбцы матрицы $\bT$ ортогональны; $\bP,\ \bQ$~--- матрицы перехода из латентного пространства в  исходные пространства; $\bE,\ \bF$~--- матрицы невязок. 

Псевдокод метода регрессии PLS приведен в алгоритме~\ref{PLSR_code}.
Алгоритм итеративно на каждом из $l$ шагов вычисляет по одному столбцу $\bt_k$, $\bp_k$, $\bq_k$ матриц $\bT$, $\bP$, $\bQ$ соответственно. После вычисления следующего набора векторов из матриц $\bX$, $\bY$ вычитаются очередные одноранговые аппроксимации. 
Первым шагом необходимо произвести нормировку столбцов исходных матриц (вычесть среднее и разделить на стандартное отклонение).
На этапе тестирования необходимо провести нормировку тестовых данных, вычислить предсказание модели~\ref{eq::model}, а затем провести обратную нормировку.

\begin{algorithm}[h]
\caption{PLSR algorithm}
\label{PLSR_code}
\begin{algorithmic}[1]
	\REQUIRE $\bX, \bY, l$;
	\ENSURE $\bT, \bP, \bQ$;
	\STATE normalize matrices $\bX$ и $\bY$ by columns
	\STATE initialize $\bu_0$ (the first column of $\bY$)
	\STATE $\bX_1 = \bX; \bY_1 = \bY$
	\FOR{$k=1,\dots, l$}
	\REPEAT
	\vspace{0.1cm}
	\STATE $\bw_k := \bX_k^{\T} \bu_{k-1} / (\bu_{k-1}^{\T} \bu_{k-1}); \quad \bw_k: = \frac{\bw_k}{\| \bw_k \|}$
	\vspace{0.1cm}
	\STATE $\bt_k := \bX_k \bw_k$
	\vspace{0.1cm}
	\STATE $\bc_k := \bY_k^{\T} \bt_k / (\bt_k^{\T} \bt_k); \quad \bc_k: = \frac{\bc_k}{\| \bc_k \|}$
	\vspace{0.1cm}
	\STATE $\bu_k := \bY_k \bc_k$
	\UNTIL{$\bt_k$ stabilizes}
	\vspace{0.1cm}
	\STATE $\bp_k:= \bX_k^{\T}\bt_k/(\bt_k^{\T}\bt_k),\ 
	\bq_k := \bY_k^{\T}\bt_k/(\bt_k^{\T}\bt_k)$
	\vspace{0.2cm}
	\STATE $\bX_{k+1} :=  \bX_k - \bt_k \bp_k^{\T}$
	\vspace{0.2cm}
	\STATE $\bY_{k + 1} :=  \bY_k - \bt_k \bq_k^{\T}$ 
	\ENDFOR
\end{algorithmic}
\end{algorithm}
Вектора $\bt_k$ и $\bu_k$ из внутреннего цикла алгоритма~\ref{PLSR_code}
содержат информацию о матрице объектов $\bX$ и матрице ответов $\bY$ соответственно. 
Блоки из шагов (6)-(7) и шагов (8)-(9)~--- аналоги алгоритма PCA для матриц $\bX$ и $\bY$~\cite{geladi1986partial}. 
Последовательное выполнение блоков позволяет учесть взаимную связь между матрицами $\bX$ и $\bY$.

Теоретическое обоснование алгоритма PLS следует из следующих утверждений.
\begin{statement}
Наилучшее описание матриц $\bX$ и $\bY$ с учётом их взаимосвязи достигается при максимизации ковариации между векторами $\bt_k$ и $\bu_k$. 
\end{statement}
Утверждение следует из равенства
\[
\text{cov} (\bt_k, \bu_k) = \text{corr} (\bt_k, \bu_k) \cdot \sqrt{\text{var}(\bt_k)} \cdot \sqrt{\text{var}(\bu_k)}.
\]
Максимизация дисперсий векторов $\bt_k$ и $\bu_k$ отвечает за сохранение информации об исходных матрицах, 
корреляция между векторами отвечает взаимосвязи между $\bX$ и~$\bY$. $\blacksquare$

Во внутреннем цикле алгоритма вычисляются нормированные вектора весов $\bw_k$ и $\bc_k$. Из данных векторов строятся матрицы весов $\bW$ и $\bC$ соответственно.

\begin{statement}
	В результате выполнения внутреннего цикла вектора $\bw_k$ и $\bc_k$ будут собственными векторами матриц $\bX_k^{\T} \bY_k \bY_k^{\T} \bX_k$ и $\bY_k^{\T} \bX_k \bX_k^{\T} \bY_k$, соответствующими максимальным собственным значениям.
	
	\begin{equation*}
	\bw_k \varpropto \bX_k^{\T} \bu_{k-1} \varpropto \bX_k^{\T} \bY_k \bc_{k-1} \varpropto \bX_k^{\T} \bY_k \bY_k^{\T} \bt_{k-1} \varpropto \bX_k^{\T} \bY_k \bY_k^{\T} \bX_k \bw_{k-1},
	\end{equation*}
	\begin{equation*}
	\bc_k \varpropto \bY_k^{\T} \bt_k \varpropto \bY_k^{\T} \bX_k \bw_k \varpropto \bY_k^{\T} \bX_k \bX_k^{\T} \bu_{k-1} \varpropto \bY_k^{\T} \bX_k \bX_k^{\T} \bY_k \bc_{k-1},
	\end{equation*}
	где символ $\varpropto$ означает равенство с точностью до мультипликативной константы. 
	\label{st::eig}
\end{statement}

Утверждение следует из того факта, что правила обновления векторов $\bw_k$, $\bc_k$ совпадают с итерацией алгоритма поиска максимального собственного значения. Данный алгоритм основан на следующем факте.

Если матрица $\mathbf{A}$ диагонализуема, $\bx$~--- некоторый вектор, то

\[
	\lim_{k \rightarrow \infty} \mathbf{A}^k \bx = \lambda_{\max}(\mathbf{A}) \cdot \mathbf{v}_{\max},
\]
где $ \lambda_{\max} (\mathbf{A})$~--- максимальное собственное значение матрицы $\mathbf{A}$, $\mathbf{v}_{\max}$~---собственный вектор матрицы $\mathbf{A}$, соответствующий $ \lambda_{\max} (\mathbf{A})$.
$\blacksquare$

\begin{statement}
Обновление векторов по шагам (6)--(9) алгоритма~\ref{PLSR_code} соответствует максимизации ковариации между векторами $\bt_k$ и $\bu_k$.
\end{statement}
\hrulefill

The maximum covariance between the vectors $\bt_k$ and $\bu_k$ is equal to the maximum eigenvalue of the matrix $\bX_k^{\T} \bY_k \bY_k^{\T} \bX_k$:
\begin{align*}
\max_{\bt_k, \bu_k}  \text{cov} (\bt_k, \bu_k)^2 &= \max_{\substack{\|\bw_k\|=1 \\ \|\bc_k\| = 1}} \text{cov} \left( \bX_k \bw_k, \bY_k \bc_k \right)^2 = \max_{\substack{\|\bw_k\|=1 \\ \|\bc_k\| = 1}} \text{cov} \left(\bc_k^{\T}  \bY_k^{\T} \bX_k \bw_k \right)^2 = \\
&= \max_{\|\bw_k\| = 1} \text{cov} \left\|\bY_k^{\T} \bX_k \bw_k \right\|^2 = \max_{\|\bw_k\| = 1} \bw_k^{\T} \bX_k^{\T} \bY_k \bY_k^{\T} \bX_k \bw_k = \\
& = \lambda_{\max} \left( \bX_k^{\T} \bY_k \bY_k^{\T} \bX_k \right),
\end{align*}
where $ \lambda_{\max} (\mathbf{A})$~is the maximum eigenvalue of the matrix $\mathbf{A}$.
Using the statement~\ref{st::eig}, we obtain the required result.
$\blacksquare$

\hrulefill

После завершения внутреннего цикла на шаге (11) вычисляются вектора $\bp_k$, $\bq_k$ проецированием столбцов матриц $\bX_k$ и $\bY_k$ на вектор $\bt_k$. Для перехода на следующий шаг необходимо вычесть из матриц $\bX_k$ и $\bY_k$ одноранговые аппроксимации $\bt_k \bp_k^{\T}$ и $\bt_k \bq_k^{\T}$
\begin{equation*}
    \bX_{k + 1} = \bX_{k} - \bt_k \bp_k^{\T} = \bX - \sum_k \bt_k \bp_k^{\T},
\end{equation*}
\begin{equation*}
    \bY_{k + 1} = \bY_{k} - \bt_k \bq_k^{\T} = \bY - \sum_k \bt_k \bq_k^{\T}.
\end{equation*}
Тогда каждый следующий вектор $\bt_k$ оказывается ортогонален всем векторам $\bt_i$, $i=1, \dots, k$.

\hrulefill

Let assume that the dimension of objects, responses and latent variables spaces are equal to 2 ($n = r = l = 2$).
Fig.~\ref{fig::PLSFigure} shows the result of the PLS algorithm in this case.
Blue and green dots represent the rows of the matrices $\bX$ and $\bY$, respectively. 
The dots were generated from a normal distribution with zero expectation. 
The covariance matrices contours of the distributions are shown in red.
The black contour is a unit circle. 
Red arrows correspond to principal components for the set of points. 
Black arrows correspond to the vectors of the matrices $\bW$ and $\bC$ from the PLS algorithm. 
The vectors $\bt_k$ and $\bu_k$ are equal to the projected matrices $\bX_k$ and $\bY_k$ on the vectors $\bw_k$ and $\bc_k$, respectively, and are denoted by black pluses. 
Taking into account the interaction between the matrices $\bX$ and $\bY$ deviates the vectors $\bw_k$ and $\bc_k$ from the principal components directions. 
The deviation of the vectors $\bw_k$ is insignificant. 
In the first iteration, $\bc_1$ is close to the principal component $\textit{pc}_1$, but the vectors $\bc_k$ in the next iterations could strongly correlate. 
The difference in the behaviour of the vectors $\bw_k$ and $\bc_k$ is associated with the deflation process. In particular we subtract from $\bY$ one-rank approximation found in the space of the matrix $\bX$.
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figs/PLSFigure.eps}
	\caption{Иллюстрация алгоритма PLS}
	\label{fig::PLSFigure}
\end{figure}

To obtain the model predictions and find the model parameters, let multiply the both sides of the equation~\eqref{eq::PLS_X} by the matrix $\bW$. Since the rows of the residual matrix  $\bE$ are orthogonal to the columns of the matrix $\bW$, we have
\[
	\bX \bW = \bT \bP^{\T} \bW.
\]
The linear transformation between objects in the input and latent spaces has the form
\begin{equation}
	\bT = \bX \bW^*,
	\label{eq::W*}
\end{equation}
where $\bW^* = \bW (\bP^{\T} \bW)^{-1}$.

The matrix of the model parameters~\ref{eq::model} could be found from equations~\eqref{eq::PLS_Y},~\eqref{eq::W*}
\begin{equation*}
    \bY = \bT \bQ^{\T} + \bE = \bX \bW^* \bQ^{\T} + \bE = \bX \bTheta + \bE.
\end{equation*}
Thus, the model parameters~\eqref{eq::model} are equal to
\begin{equation}
    \bTheta = \bW (\bP^{\T} \bW)^{-1} \bQ^{\T}.
    \label{eq::model_parameters}
\end{equation}

To find the model predictions during the testing we have to
\begin{itemize}
	\item normalize the test data;
	\item compute the prediction of the model using the linear transformation with the matrix $\bTheta$ from~\eqref{eq::model_parameters};
	\item perform the inverse normalization.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Time series of energy consumption contain hourly records (total of 52512 observations). 
A row of the matrix $\bX$ is the local history of the signal for one week $n = 24 \times 7$. A row of the matrix $\bY$ is the local forecast of energy consumption for the next 24 hours $r = 24$. In this case, the matrices $\bX$ and $\bY$ are autoregressive matrices.

In the case of the ECoG data, the matrix $\bX$ consists of the spatial-temporal representation of voltage time series, and the matrix $\bY$ contains information about the position of the hand.
The generation process of the matrix $\bX$ from the voltage values described in~\cite{gasanov2017pls}. 
Feature description in each time moment has dimension equal to $864$, the hand position is described by the coordinates along three axes. 
An example of voltage data samples with the different channels and corresponding spatial coordinates of the hand are shown in Fig.~\ref{fig::ecog_data}.
To predict the position of the hand in the next moments we used an autoregressive approach.
One object consists of a feature description in a few moments. 
The answer is the hand position in the next moments of time.
The task is to predict the hand position in the next few moments of time.

\begin{figure}
	\includegraphics[width=\linewidth]{figs/ecog_data.eps}
	\caption{Пример данных ECoG. Слева изображены данные напряжения, снятые с нескольких каналов, справа~--- координаты руки по трём осям. The ECoG data example. On the left voltage data taken from multiple channels is shown, on the right there are coordinates of the hand along three axes. \textcolor{blue}{как-то неочень}}
	\label{fig::ecog_data}
\end{figure}

Введём среднеквадратичную ошибку для некоторых матриц $\mathbf{A} = [a_{ij}]$ и $\mathbf{B} = [b_{ij}]$
\[
\text{MSE} (\mathbf{A}, \mathbf{B}) = \sum_{i,j} (a_{ij} - b_{ij})^2.
\]
Для оценивания качества аппроксимации вычисляется значение нормированной среднеквадратичной ошибки
\begin{equation}
\text{NMSE}(\bY,  \mathbf{\hat{Y}}) = \frac{\text{MSE} (\bY, \mathbf{\hat{Y}})}{\text{MSE} (\bY, \mathbf{\bar{Y}})},
\label{eq::nmse}
\end{equation}
где $\mathbf{\hat{Y}}$~--- прогноз модели, $\mathbf{\bar{Y}}$~--- константный прогноз средним значением по столбцам матрицы.

\textcolor{blue}{
We introduce the mean-squared error for matrices $\mathbf{A} = [a_{ij}]$ and $\mathbf{B} = [b_{ij}]$ 
\[
\text{MSE} (\mathbf{A}, \mathbf{B}) = \sum_{i,j} (a_{ij} - b_{ij})^2.
\]
To estimate the prediction quality, we compute the normalized MSE 
\begin{equation}
\text{NMSE}(\bY,  \mathbf{\hat{Y}}) = \frac{\text{MSE} (\bY, \mathbf{\hat{Y}})}{\text{MSE} (\bY, \mathbf{\bar{Y}})},
\label{eq::nmse}
\end{equation}
where $\mathbf{\hat{Y}}$ is the model outcome, $\mathbf{\bar{Y}}$ is the constant forecast by the average value over the columns of the matrix.}

\subsection{Energy consumption dataset}

To find the optimal dimensionality $l$ of the latent space, the energy consumption dataset was divided into training and validation parts. 
The training data consists of $700$ objects, the validation data is of $370$ ones. The dependence of the normalized mean-squared error~\eqref{eq::nmse} on the latent space with dimensionality $l$ is shown in Fig.~\ref{fig::energy_n_comp}. 
First, the error drops sharply with increasing the latent space dimensionality and then changes slightly.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.75\linewidth]{figs/energy_n_comp}
	\caption{The energy consumption forecast by the PLS algorithm (the latent space dimensionality is equal to $l$=14).}
	\label{fig::energy_n_comp}
\end{figure}

\hrulefill

The error achieves the minimum value for $l=14$. 
Let build a forecast of energy consumption for a given $l$. 
The result is shown in Fig.~\ref{fig::energy_prediction}. 
The PLS algorithm restored the autoregressive dependence and found the daily seasonality.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.95\textwidth]{figs/energy_prediction}
	\caption{Зависимость ошибки от размерности латентного пространства для данных потребления электроэнергии. The dependence of the error on the latent space dimensionality for the energy consumption dataset.}
	\label{fig::energy_prediction}
\end{figure}

\subsection{ECoG dataset}

На Рис.~\ref{fig::ecog_n_comp} представлена зависимость нормированной квадратичной ошибки~\eqref{eq::nmse} от размерности латентного пространства. Ошибка аппроксимации меняется незначительно при $l > 5$.
Таким образом совместное описание пространственно-временного спектрального представления объектов и пространственного положения руки может быть представлено вектором размерности $l \ll n$.
Зафиксируем $l = 5$. 
Пример аппроксимации положения руки изображен на Рис.~\ref{fig::ecog_prediction}. 
Сплошными линиями изображены истинные координаты руки по всем осям, пунктирными линиями показана аппроксимация методом PLS.

\textcolor{blue}{
Fig.~\ref{fig::ecog_n_comp} shows the dependence of the normalized quadratic error~\eqref{eq::nmse} on the latent space dimensionality. The approximation error changes slightly when $l > 5$.
Thus, the joint spatial-temporal spectrum representation of objects and the spatial position of the hand can be represented by a vector of dimensionality $l \ll n$.
Let us fix $l = 5$. 
An example of the approximation of the hand position is shown in Fig.~\ref{fig::ecog_prediction}. 
Solid lines represent the true coordinates of the hand on all axes, the dotted lines show the approximation by the PLS method.}
 
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.75\linewidth]{figs/ecog_n_comp}	
	\caption{Зависимость ошибки от размерности латентного пространства для данных ECoG. The dependence of the error on the latent space dimensionality for the ECoG dataset.}
	\label{fig::ecog_n_comp}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{figs/ecog_prediction}
	\caption{Прогноз движения руки данных ECoG алгоритмом PLS при размерности латентного пространства $l=5$. The forecast of 
	the hand movement by the PLS algorithm when the latent space dimensionality is equal to $l$=5.}
	\label{fig::ecog_prediction}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
В данной работе предложен подход к решению задачи декодирования временных рядов и прогнозирования. 
Алгоритм частичных наименьших квадратов позволяет построить векторный прогноз целевой переменной. 
Латентное пространство содержит информацию об объектах и ответах и снижает размерности исходных матриц на порядки. 
Вычислительный эксперимент продемонстрировал применимость предложенного метода к задачам прогнозирования временных рядов объёмов потребления электроэнергии и проектирования нейрокомпьютерного интерфейса.

\textcolor{blue}{
In the paper we proposed the approach for solving the problem of decoding and forecasting time-series. 
The algorithm of partial least squares allows to build a target variable vector prediction. 
The latent space contains information about the objects and the answers and reduces the dimensions of the source (? initial) matrices by orders of magnitude. 
The computational experiment demonstrated the applicability of the proposed method to the tasks of electricity consumption forecasting and brain-computer interface designing.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{СПИСОК ЛИТЕРАТУРЫ}

%\nocite{*}

\bibliographystyle{unsrt}
\bibliography{papers_pls}


\end{document}
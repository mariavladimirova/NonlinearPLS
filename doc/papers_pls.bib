@article{Chong2005,
abstract = {Variable selection is one of the important practical issues for many scientific engineers. Although the PLS (partial least squares) regression combined with the VIP (variable importance in the projection) scores is often used when the multicollinearity is present among variables, there are few guidelines about its uses as well as its performance. The purpose of this paper is to explore the nature of the VIP method and to compare with other methods through computer simulation experiments. We design 108 experiments where observations are generated from true models considering four factors-the proportion of the number of relevant predictors, the magnitude of correlations between predictors, the structure of regression coefficients, and the magnitude of signal to noise. Confusion matrix is adopted to evaluate the performance of PLS, the Lasso, and stepwise method. We also discuss the proper cutoff value of the VIP method to increase its performance. Some practical hints for the use of the VIP method are given as simulation results. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Chong, Il Gyo and Jun, Chi Hyuck},
doi = {10.1016/j.chemolab.2004.12.011},
file = {:Users/m/Downloads/chong2005.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Multicollinearity,Partial least squares regression,Stepwise regression,The lasso,VIP (Variable Importance in the Projection) scores,Variable selection},
number = {1},
pages = {103--112},
pmid = {17512828},
title = {{Performance of some variable selection methods when multicollinearity is present}},
volume = {78},
year = {2005}
}
@article{Xuefeng2010,
abstract = {A novel hybrid artificial neural network (HANN) integrating error back propagation algorithm (BP) with partial least square regression (PLSR) was proposed to overcome two main flaws of artificial neural network (ANN), i.e. tendency to overfitting and difficulty to determine the optimal number of the hidden nodes. Firstly, single-hidden-layer network consisting of an input layer, a single hidden layer and an output layer is selected by HANN. The number of the hidden-layer neurons is determined according to the number of the modeling samples and the number of the neural network parameters. Secondly, BP is employed to train ANN, and then the hidden layer is applied to carry out the nonlinear transformation for independent variables. Thirdly, the inverse function of the output-layer node activation function is applied to calculate the expectation of the output-layer node input, and PLSR is employed to identify PLS components from the nonlinear transformed variables, remove the correlation among the nonlinear transformed variables and obtain the optimal relationship model of the nonlinear transformed variables with the expectation of the output-layer node input. Thus, the HANN model is developed. Further, HANN was employed to develop naphtha dry point soft sensor and the most important intermediate product concentration (i.e. 4-carboxybenzaldehyde concentration) soft sensor in p-xylene (PX) oxidation reaction due to the fact that there exist many factors having nonlinear effect on them and significant correlation among their factors. The results of two HANN applications show that HANN overcomes overfitting and has the robust character. And, the predicted squared relative errors of two optimal HANN models are all lower than those of two optimal ANN models and the mean predicted squared relative errors of HANN are lower than those of ANN in two applications. {\textcopyright} 2010 Elsevier B.V.},
author = {Xuefeng, Yan},
doi = {10.1016/j.chemolab.2010.07.002},
file = {:Users/m/Desktop/pls/literature/nonlinear/xuefeng2010.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Artificial neural network,Correlation,Error back propagation,Partial least square regression,Soft sensor},
number = {2},
pages = {152--159},
publisher = {Elsevier B.V.},
title = {{Hybrid artificial neural network based on BP-PLSR and its application in development of soft sensors}},
url = {http://dx.doi.org/10.1016/j.chemolab.2010.07.002},
volume = {103},
year = {2010}
}
@article{Mcavovt1992,
author = {Mcavovt, J and Process, Chemical},
file = {:Users/m/Desktop/pls/literature/nonlinear/qin1992.pdf:pdf},
number = {4},
pages = {379--391},
title = {{USING}},
volume = {16},
year = {1992}
}
@article{Yan2003,
abstract = {A novel genetic algorithm (GA) including chaotic variable named chaos-genetic algorithm (CGA) was proposed. Due to the nature of chaotic variable, i.e. pseudo-randomness, ergodicity and irregularity, the evolutional process of CGA makes the individuals of subgenerations distributed ergodically in the defined space and circumvents the premature of the individuals of subgenerations. The performance of CGA was demonstrated through two examples and compared with that by the traditional genetic algorithms (TGA). The results showed the superior performances of CGA over TGA, and moreover, the probability of finding the global optimal value by using CGA is larger than that by using TGA. To illustrate the performance of CGA further, it was employed to optimize the operating conditions of aromatic hydrocarbon isomerization (AHI) process modeled by the radial basis functions (RBF) coupled with partial least squares (PLS) approach. Satisfactory results were obtained. Further, a generalized methodology, which employs RBF-PLS approach to model the complex chemical process based upon practical observation data and subsequently applies CGA to find the optimal operating conditions, was suggested too. ?? 2003 Elsevier Science Ltd. All rights reserved.},
author = {Yan, Xuefeng F. and Chen, Dezhao Z. and Hu, Shangxu X.},
doi = {10.1016/S0098-1354(03)00074-7},
file = {:Users/m/Desktop/pls/literature/nonlinear/yan2003.pdf:pdf},
isbn = {0098-1354},
issn = {00981354},
journal = {Computers and Chemical Engineering},
keywords = {Chaos-genetic algorithms,Chaotic variable,Model,Optimization,Partial least squares,Radial basis functions},
number = {10},
pages = {1393--1404},
title = {{Chaos-genetic algorithms for optimizing the operating conditions based on RBF-PLS model}},
volume = {27},
year = {2003}
}
@article{Malthouse1997,
abstract = {All fights reserved PII: S00981354(96)003110 00981354 97 17.00 + 0.00    EC ,* AC Tamhane and RSH Mah Northwestern },
author = {Malthouse, E.C. and Tamhane, A.C. and Mah, R.S.H.},
doi = {10.1016/S0098-1354(96)00311-0},
file = {:Users/m/Desktop/pls/literature/nonlinear/pls non lineare.pdf:pdf},
issn = {00981354},
journal = {Computers {\&} Chemical Engineering},
number = {8},
pages = {875--890},
pmid = {30601},
title = {{Nonlinear partial least squares}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0098135496003110},
volume = {21},
year = {1997}
}
@article{Frank1990,
abstract = {Frank, I.E., 1990. A nonlinear PLS model. Chemometrics and Intelligent Laboratory Systems, 8: 109-119. A nonlinear extension of the PLS (partial least squares regression) method is introduced. The algorithm connects the predictor and response latent variables by a smooth but otherwise unrestricted nonlinear function. Similarities and differences between the linear and nonlinear PLS models are discussed. The performance of the new nonlinear PLS method is illustrated on three chemical data sets with different covariance structures. ?? 1990.},
author = {Frank, Ildiko E.},
doi = {10.1016/0169-7439(90)80128-S},
file = {:Users/m/Desktop/pls/literature/nonlinear/frank1990.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
number = {2},
pages = {109--119},
pmid = {1280},
title = {{A nonlinear PLS model}},
volume = {8},
year = {1990}
}
@article{Zhou2007,
abstract = {In the present study a new version of nonlinear partial least-square method based on artificial neural network transformation (ANN-NLPLS) has been proposed. This algorithm firstly transforms the training descriptors into the hidden layer outputs using the universal nonlinear mapping carried by an artificial neural network, and then utilizes PLS to relate the outputs of the hidden layer to the bioactivities. The weights between the input and hidden layers are optimized by a particle swarm optimization (PSO) method using the criterion of minimized model error via PLS modeling. An F-statistic is introduced to determine automatically the number of PLS components during the weight optimization. The performance is assessed using a simulated data set and two quantitative structure-activity relation (QSAR) data sets. Results of these three data sets demonstrate that ANN-NLPLS offers enhanced capacity in modeling nonlinearity while circumventing the overfitting frequently involved in nonlinear modeling. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Zhou, Yan Ping and Jiang, Jian Hui and Lin, Wei Qi and Xu, Lu and Wu, Hai Long and Shen, Guo Li and Yu, Ru Qin},
doi = {10.1016/j.talanta.2006.05.058},
file = {:Users/m/Desktop/pls/literature/zhou2007PLS{\_}ANN.pdf:pdf},
isbn = {0039-9140},
issn = {00399140},
journal = {Talanta},
keywords = {2-Aryl(heteroaryl)-2,5-dihydropyrazolo[4,3-c]quinolin-3-(3H)-ones,Antitumor agents,Artificial neural network transformation-based nonlinear partial least-square,Particle swarm optimization,QSAR},
number = {2},
pages = {848--853},
pmid = {19071384},
title = {{Artificial neural network-based transformation for nonlinear partial least-square regression with application to QSAR studies}},
volume = {71},
year = {2007}
}
@article{Geladi1988,
abstract = {The growing awareness of the need for non-deterministic and distribution-free (soft) models combined with an iterative algorithm for finding latent variables led to the construction of partial least squares models. There have been separate developments in the humanistic and the natural sciences, with stress on different aspects and a different terminology. The historical development is described and some key topics are explained.},
author = {Geladi, Paul},
doi = {http://doi.wiley.com/10.1002/cem.1180020403},
file = {:Users/m/Desktop/pls/literature/geladi1988.pdf:pdf},
isbn = {1099-128X},
issn = {1099-128X},
journal = {Journal of Chemometrics},
keywords = {History of PLS,partial least squares models},
number = {January},
pages = {231--246},
title = {{Notes on the history and nature of partial least squares (PLS) modelling}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cem.1180020403/abstract},
volume = {2},
year = {1988}
}
@article{Hoskuldsson1988,
abstract = {In this paper we develop the mathematical and statistical structure of PLS regression. We show the PLS regression algorithm and how it can be interpreted in model building. The basic mathematical principles that lie behind two block PLS are depicted. We also show the statistical aspects of the PLS method when it is used for model building. Finally we show the structure of the PLS decompositions of the data matrices involved},
author = {H{\"{o}}skuldsson, Agnar},
doi = {10.1002/cem.1180020306},
file = {:Users/m/Desktop/pls/literature/hskuldsson1988.pdf:pdf},
isbn = {1099-128X},
issn = {1099-128X},
journal = {Journal of Chemometrics},
keywords = {component regression,covariance decomposition,partial least squares},
number = {August 1987},
pages = {581--591},
title = {{PLS regression.}},
volume = {2},
year = {1988}
}
@article{Bulut2014,
author = {Bulut, Elif and Egrioglu, Erol},
doi = {10.5923/j.ajis.20140404.05},
file = {:Users/m/Desktop/pls/literature/PLS{\_}ElmamNN.pdf:pdf},
keywords = {19,activation functions,based on logistic activation,based on radial bases,elman neural network,feed forward neural network,function and particle swarm,optimization methods,partial least squares regression,prediction,propose non-linear pls method,xufeng,zhou et al},
number = {4},
pages = {154--158},
title = {{A New Partial Least Square Method Based on Elman Neural Network}},
volume = {4},
year = {2014}
}
@article{,
file = {:Users/m/Desktop/pls/literature/main/novitskiy.pdf:pdf},
pages = {1--10},
title = {{Выбор признаков в задаче авторегрессионного прогнозирования биомедицинских сигналов *}},
year = {2017}
}
@article{Ng2013,
abstract = {Partial Least Squares (PLS) is a widely used technique in chemometrics, especially in the case where the number of independent variables is significantly larger than the number of data points. There are many articles on PLS [HTF01, GK86] but the mathematical details of PLS do not always come out clearly in these treatments. This paper is an attempt to describe PLS in precise and simple mathematical terms. 2},
author = {Ng, Kee Siong},
doi = {10.1.1.352.4447},
file = {:Users/m/Desktop/pls/literature/main/pls.pdf:pdf},
pages = {1--10},
title = {{A Simple Explanation of Partial Least Squares}},
url = {http://users.rsise.anu.edu.au/{~}kee/pls.pdf},
year = {2013}
}
@article{Rosipal2011,
abstract = {In many areas of research and industrial situations, including many data analytic problems in chemistry, a strong nonlinear relation between different sets of data may exist. While linear models may be a good simple approximation to these problems, when nonlinearity is severe they often perform unacceptably. The nonlinear partial least squares (PLS) method was developed in the area of chemical data analysis. A specific feature of PLS is that relations between sets of observed variables are modeled by means of latent variables usually not directly observed and measured. Since its introduction, two methodologically different concepts of fitting existing nonlinear rela- tionships initiated development of a series of different nonlinear PLS models. General principles of the two concepts and representative models are reviewed in this chapter. The aim of the chapter is two-fold i) to clearly summarize achieved results and thus ii) to motivate development of new computationally efficient nonlinear PLS models with better performance and good interpretability.},
author = {Rosipal, Roman},
doi = {10.4018/978-1-61520-911-8.ch009},
file = {:Users/m/Desktop/pls/literature/main/roman{\_}rosipal{\_}nlpls{\_}an{\_}overview.pdf:pdf},
isbn = {9781615209118},
journal = {Chemoinformatics and Advanced Machine Learning Perspectives: Complex Computational Methods and Collaborative Techniques},
keywords = {kernel learning,nonlinear mapping,partial least squares},
pages = {169--189},
title = {{Nonlinear partial least squares: An overview}},
url = {http://aiolos.um.savba.sk/{~}roman/Papers/npls{\_}book11.pdf{\%}5Cnpapers2://publication/uuid/55966E8F-2346-4326-B57E-3A1F47835189},
year = {2011}
}
@article{Wold1989,
abstract = {The linear two block predictive PLS model (PPLS2) is often used to model the relation between two data matrices, X and Y. Applications include multivariate calibration, quantitative structure-activity relationships (QSAR), and process optimization. In each PPLS2 model dimension the matrices X and Y are decomposed as bilinear products plus residual matrices: X = tp′ + E Y = uq′ + F. In addition, a linear model is assumed to relate the score vectors t and u (h denotes residuals): u = bt + h. This allows Y to be modeled by t and q as: Y = tq′b + f{\{}hook{\}}*. In the present work the linear PPLS2 model is extended to the case when the inner model relating the block scores u and t is nonlinear (h is a vector of residuals): u = f{\{}hook{\}}(t) + h. An algorithm is outlined for the model where the inner relation is a quadratic polynomial: u = c0 + c1t + c2t2 + h. This will be referred to as the QPLS2 model (standing for quadratic PLS with two blocks). Applications to cosmetics qualimetrics and a drug structure-activity relationship are used as illustrations. {\textcopyright} 1989.},
author = {Wold, Svante and Kettaneh-Wold, Nouna and Skagerberg, Bert},
doi = {10.1016/0169-7439(89)80111-X},
file = {:Users/m/Desktop/pls/literature/main/wold1989.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
number = {1-2},
pages = {53--65},
pmid = {1277},
title = {{Nonlinear PLS modeling}},
volume = {7},
year = {1989}
}
@article{Rosipal2006,
abstract = {Partial Least Squares (PLS) is a wide class of methods for modeling relations between sets of observed variables by means of latent variables. It comprises of regression and classification tasks as well as dimension reduction techniques and modeling tools. The underlying assumption of all PLS methods is that the observed data is generated by a system or process which is driven by a small number of latent (not directly observed or measured) variables. Projections of the observed data to its latent structure by means of PLS was developed by Herman Wold and coworkers [48, 49, 52]. PLS has received a great amount of attention in the field of chemomet- rics. The algorithm has become a standard tool for processing a wide spectrum of chemical data problems. The success of PLS in chemometrics resulted in a lot of applications in other scientific areas including bioinformatics, food re- search, medicine, pharmacology, social sciences, physiology–to name but a few [28, 25, 53, 29, 18, 22]. This chapter introduces the main concepts of PLS and provides an overview of its application to different data analysis problems. Our aim is to present a concise introduction, that is, a valuable guide for anyone who is concerned with data analysis.},
author = {Rosipal, Roman and Kramer, Nicole},
doi = {10.1007/11752790_2},
file = {:Users/m/Desktop/pls/literature/main/RosipalKramer{\_}2006{\_}OverviewAndRecentAdvancesInPLS.pdf:pdf},
isbn = {9783540341376},
issn = {03029743},
journal = {C. Saunders et al. (Eds.): SLSFS 2005, LNCS 3940},
pages = {34--51},
pmid = {238094700002},
title = {{Overview and Recent Advances in Partial Least Squares}},
year = {2006}
}
@article{Lu2004,
annote = {NULL},
author = {Lu, Wen-Cong and Chen, Nian-Yi and Li, Guo-Zheng and Yang, Jie},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2004 - Multitask Learning Using Partial Least Squares Method.pdf:pdf},
journal = {Proceedings of the Seventh International Conference on Information Fusion; International Society of Information Fusion},
keywords = {feature,multitask learning,multivariate calibration,partial least squares,selection},
pages = {79--84},
title = {{Multitask Learning Using Partial Least Squares Method}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.3907{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://www.fusion2004.foi.se/papers/IF04-0079.pdf},
volume = {1},
year = {2004}
}
@book{Adolph,
annote = {NULL},
author = {Adolph, Karen E},
booktitle = {Action As an Organizer of Learning and Development},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Adolph - Unknown - Learning to learn.pdf:pdf},
isbn = {9781461375272},
title = {{Learning to learn}}
}
@article{Caruana2003,
abstract = {In supervised learning variable selection is used to find a subset of the available inputs that accurately predict the output. This paper shows that some of the variables that variable selection discards can beneficially be used as extra outputs for inductive transfer. Using discarded input variables as extra outputs forces the model to learn mappings from the variables that were selected as inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available to the model that is being trained on the main output, often resulting in improved performance on that main output. We present three synthetic problems (two regression problems and one classification problem) where performance improves if some variables discarded by variable selection are used as extra outputs. We then apply variable selection to two real problems (DNA splice-junction and pneumonia risk prediction) and demonstrate the same effect: using some of the discarded input variables as extra outputs yields somewhat better performance on both of these problems than can be achieved by variable selection alone. This new approach enhances the benefit of variable selection by allowing the learner to benefit from variables that would otherwise have been discarded by variable selection, but without suffering the loss in performance that occurs when these variables are used as inputs.},
annote = {NULL},
author = {Caruana, Rich and de Sa, Virginia R.},
doi = {10.1162/153244303322753652},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Caruana, de Sa - 2003 - Benefitting from the Variables that Variable Selection Discards.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {inductive transfer,multitask learning,output variable selection},
number = {7-8},
pages = {1245--1264},
title = {{Benefitting from the Variables that Variable Selection Discards}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2003}
}
@article{Abdi2003,
abstract = {PLS regression is a recent technique that generalizes and combines features from principal component analysis and multiple regression. Its goal is to predict or analyze a set of dependent variables from a set of independent variables or predictors. This prediction is achieved by extracting from the predictors a set of orthogonal factors called latent variables which have the best predictive power.},
annote = {NULL},
author = {Abdi, Herv{\'{e}}},
doi = {http://dx.doi.org/10.4135/9781412950589.n690},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Abdi - 2003 - Partial Least Squares (PLS) Regression.pdf:pdf},
isbn = {9781412950589},
issn = {15315487},
journal = {Encyclopedia for research methods for the social sciences},
pages = {792--795},
pmid = {20539106},
title = {{Partial Least Squares (PLS) Regression}},
year = {2003}
}
@article{Lehky2014,
abstract = {We have calculated the intrinsic dimensionality of visual object representations in anterior inferotemporal (AIT) cortex, based on responses of a large sample of cells stimulated with photographs of diverse objects. As dimensionality was dependent on data set size, we determined asymptotic dimensionality as both the number of neurons and number of stimulus image approached infinity. Our final dimensionality estimate was 93 (SD: ± 11), indicating that there is basis set of approximately a hundred independent features that characterize the dimensions of neural object space. We believe this is the first estimate of the dimensionality of neural visual representations based on single-cell neurophysiological data. The dimensionality of AIT object representations was much lower than the dimensionality of the stimuli. We suggest that there may be a gradual reduction in the dimensionality of object representations in neural populations going from retina to inferotemporal cortex, as receptive fields become increasingly complex. Introduction},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1309.2848v1},
author = {Lehky, Sidney R. and Kiani, Roozbeh and Esteky, Hossein and Tanaka, Keiji},
doi = {10.1162/NECO},
eprint = {1309.2848v1},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Lehky et al. - 2014 - Dimensionality of object representations in monkey inferotemporal cortex.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural computation},
number = {10},
pages = {1840--1872},
pmid = {25602775},
title = {{Dimensionality of object representations in monkey inferotemporal cortex}},
volume = {1872},
year = {2014}
}
@article{Varnek2012,
abstract = {This paper is focused on modern approaches to machine learning, most of which are as yet used infrequently or not at all in chemoinformatics. Machine learning methods are characterized in terms of the "modes of statistical inference" and "modeling levels" nomenclature and by considering different facets of the modeling with respect to input/ouput matching, data types, models duality, and models inference. Particular attention is paid to new approaches and concepts that may provide efficient solutions of common problems in chemoinformatics: improvement of predictive performance of structure-property (activity) models, generation of structures possessing desirable properties, model applicability domain, modeling of properties with functional endpoints (e.g., phase diagrams and dose-response curves), and accounting for multiple molecular species (e.g., conformers or tautomers).},
annote = {NULL},
author = {Varnek, Alexandre and Baskin, Igor},
doi = {10.1021/ci200409x},
file = {:Users/m/Library/Application Support/Mendeley Desktop/Downloaded/Varnek, Baskin - 2012 - Machine learning methods for property prediction in chemoinformatics Quo Vadis.pdf:pdf},
isbn = {1549-9596},
issn = {15499596},
journal = {Journal of Chemical Information and Modeling},
number = {6},
pages = {1413--1437},
pmid = {22582859},
title = {{Machine learning methods for property prediction in chemoinformatics: Quo Vadis?}},
volume = {52},
year = {2012}
}
